
    # mean_from_these = []
    # for training_ih, training_point_1 in enumerate(data_train_norm):
    #     distances_from_training_point_1_to_all_others = []
    #     for training_jh, training_point_2 in enumerate(data_train_norm):
    #         # print(dist_euclidean(training_point_1, training_point_2))
    #         distances_from_training_point_1_to_all_others.append(dist_euclidean(training_point_1, training_point_2))
    #     # print(distances_from_training_point_1_to_all_others)
    #     idx = np.argpartition(distances_from_training_point_1_to_all_others, average_number_neighbors - 1)
    #     kth_smallest_distance = distances_from_training_point_1_to_all_others[idx[average_number_neighbors - 1]]
    #     # sorted_distances = distances_from_training_point_1_to_all_others.sort()
    #     # print(sorted_distances)
    #     mean_from_these.append(kth_smallest_distance)
    # print(np.average(mean_from_these))

    # -- GT, version 1 -- #
    # d_true_training = distance.cdist(data_train_norm, data_train_norm, 'euclidean')
    # d_true_training.sort(axis=1)
    # d_ball = np.mean(d_true_training[:, average_number_neighbors - 1])
    # print("d_ball", d_ball)

    # w_true_test_training = csc_matrix((n_test, n_train), dtype=np.bool).todense()
    # print("iniiiit")
    # for testing_ith, testing_point in enumerate(data_test_norm):
    #     for training_jth, training_point in enumerate(data_train_norm):
    #         w_true_test_training[testing_ith, training_jth] = dist_euclidean(testing_point, training_point) < d_ball

    # -- GT, version 2: batching only testing -- #
    d_true_training = distance.cdist(data_train_norm, data_train_norm, 'euclidean')
    # d_true_test_training = distance.cdist(data_test_norm, data_train_norm, 'euclidean')
    d_true_training.sort(axis=1)
    d_ball = np.mean(d_true_training[:, average_number_neighbors - 1])
    print("d_ball", d_ball)

    number_of_splits_testing = 10
    w_true_test_training = csc_matrix((n_test, n_train), dtype=np.bool).todense()
    # print(w_true_test_training)
    data_test_norm_chunks = np.array_split(data_test_norm, number_of_splits_testing)
    test_chunk_size = data_test_norm_chunks[0].shape[0]
    # print(test_chunk_size)  # test_chunk_size * test_chunk_ith + 0/1/2
    for test_chunk_ith, test_chunk in enumerate(data_test_norm_chunks):
        d_true_test_training_chunk = distance.cdist(test_chunk, data_train_norm, 'euclidean')
        w_true_test_training[test_chunk_size * test_chunk_ith:test_chunk_size * test_chunk_ith + test_chunk_size, :] = d_true_test_training_chunk < d_ball
    print(w_true_test_training)
    print(w_true_test_training.shape)


    # -- GT, version 3: batching both training and testing -- #
    # mean_from_these = []
    # for training_ih, training_point_1 in enumerate(data_train_norm):
    #     distances_from_training_point_1_to_all_others = []
    #     for training_jh, training_point_2 in enumerate(data_train_norm):
    #         # print(dist_euclidean(training_point_1, training_point_2))
    #         distances_from_training_point_1_to_all_others.append(dist_euclidean(training_point_1, training_point_2))
    #     # print(distances_from_training_point_1_to_all_others)
    #     idx = np.argpartition(distances_from_training_point_1_to_all_others, average_number_neighbors - 1)
    #     kth_smallest_distance = distances_from_training_point_1_to_all_others[idx[average_number_neighbors - 1]]
    #     # sorted_distances = distances_from_training_point_1_to_all_others.sort()
    #     # print(sorted_distances)
    #     mean_from_these.append(kth_smallest_distance)
    # print(np.average(mean_from_these))


    # number_of_splits_testing = 30
    # w_true_test_training = csc_matrix((n_test, n_train), dtype=np.bool).todense()
    # # print(w_true_test_training)
    # data_test_norm_chunks = np.array_split(data_test_norm, number_of_splits_testing)
    # test_chunk_size = data_test_norm_chunks[0].shape[0]
    # # print(test_chunk_size)  # test_chunk_size * test_chunk_ith + 0/1/2
    # for test_chunk_ith, test_chunk in enumerate(data_test_norm_chunks):
    #     d_true_test_training_chunk = distance.cdist(test_chunk, data_train_norm, 'euclidean')
    #     w_true_test_training[test_chunk_size * test_chunk_ith:test_chunk_size * test_chunk_ith + test_chunk_size,
    #     :] = d_true_test_training_chunk < d_ball
    # print(w_true_test_training)
    # print(w_true_test_training.shape)


    # w_true_test_training = (d_true_test_training < d_ball).astype(np.int)
    # print("w_true_test_training", w_true_test_training)

    # calculate_ground_truth_as_matlab(data_train_norm, data_test_norm, average_number_neighbors)

















==============================




   for hamming_distance_used_to_test_against in range(0, max_hamming_distance_tested):
        # -- Calculate Hamming distance between binary testing set and binary training set: batching testing set -- #
        number_of_batches = 10
        u_testing_batches = np.array_split(u_testing, number_of_batches)
        retrieved_good_pairs = 0
        retrieved_pairs = 0
        for u_bth, u_testing_batch in enumerate(u_testing_batches):
            d_hamm_batch = n_bits * distance.cdist(u_testing_batch, u_training, 'hamming')
            indices_pairs_of_good_pairs_in_d_hamm_batch = zip(*np.where(d_hamm_batch < hamming_distance_used_to_test_against + 0.00001))
            test = [(row, (u_testing.shape[0] / number_of_batches) * u_bth + row) for (row, column) in indices_pairs_of_good_pairs_in_d_hamm_batch]
            print(test, u_bth)
            retrieved_good_pairs_per_batch = sum([w_true_test_training[(u_testing.shape[0] / number_of_batches) * u_bth + row][column] for (row, column) in indices_pairs_of_good_pairs_in_d_hamm_batch])
            retrieved_good_pairs = retrieved_good_pairs + retrieved_good_pairs_per_batch
            retrieved_pairs_per_batch = len(indices_pairs_of_good_pairs_in_d_hamm_batch)
            retrieved_pairs = retrieved_pairs + retrieved_pairs_per_batch
        # d_hamm = sh_param.n_bits * distance.cdist(u_testing, u_training, 'hamming')  # np.array_split(u_testing, 2)[0]
        print("# -- DONE D_HAMM(TESTING, TRAINING) -- #")

        # indices_pairs_of_good_pairs_in_d_hamm = zip(*np.where(d_hamm < hamming_distance_used_to_test_against + 0.00001))
        # retrieved_good_pairs = sum([w_true_test_training[row][column] for (row, column) in indices_pairs_of_good_pairs_in_d_hamm])
        # retrieved_pairs = len(indices_pairs_of_good_pairs_in_d_hamm)

        # print("BEFORE")
        # print "retrieved_good_pairs", retrieved_good_pairs
        # print "retrieved_pairs", retrieved_pairs
        # print score_precision[hamming_distance_used_to_test_against][0]
        score_precision[hamming_distance_used_to_test_against][0] = retrieved_good_pairs / (retrieved_pairs * 1.0)
        # print score_precision[hamming_distance_used_to_test_against][0]
        # print "AFTER"
        score_recall[hamming_distance_used_to_test_against][0] = retrieved_good_pairs / (total_good_pairs * 1.0)
    return score_precision.T, score_recall.T



















________________________________________________________________
def evaluate_for_bits_category(w_true_test_training, d_hamm, max_hamming_distance_tested):
    total_good_pairs = w_true_test_training.sum()

    score_precision = np.zeros((max_hamming_distance_tested, 1))
    score_recall = np.zeros((max_hamming_distance_tested, 1))

    for hamming_distance_used_to_test_against in range(0, max_hamming_distance_tested):
        retrieved_good_pairs = 0
        retrieved_pairs = 0
        for row, testing_point_from in enumerate(d_hamm):
            for column, training_point_to in enumerate(testing_point_from):
                if(d_hamm[row, column] < hamming_distance_used_to_test_against + 0.00001):
                    retrieved_good_pairs += w_true_test_training[row, column]
                    retrieved_pairs += 1
        # indices_pairs_of_good_pairs_in_d_hamm = zip(*np.where(d_hamm < hamming_distance_used_to_test_against + 0.00001))
        # retrieved_good_pairs = sum([w_true_test_training[row][column] for (row, column) in indices_pairs_of_good_pairs_in_d_hamm])
        # retrieved_pairs = len(indices_pairs_of_good_pairs_in_d_hamm)

        # print("BEFORE")
        # print "retrieved_good_pairs", retrieved_good_pairs
        # print "retrieved_pairs", retrieved_pairs
        # print score_precision[hamming_distance_used_to_test_against][0]
        score_precision[hamming_distance_used_to_test_against][0] = retrieved_good_pairs / (retrieved_pairs * 1.0)
        # print score_precision[hamming_distance_used_to_test_against][0]
        # print "AFTER"
        score_recall[hamming_distance_used_to_test_against][0] = retrieved_good_pairs / (total_good_pairs * 1.0)
    return score_precision.T, score_recall.T



def calculate_ground_truth_as_matlab(data_train_norm, data_test_norm, average_number_neighbors):
    d_true_training = distance.cdist(data_train_norm, data_train_norm, 'euclidean')
    d_true_test_training = distance.cdist(data_test_norm, data_train_norm, 'euclidean')
    d_true_training.sort(axis=1)
    d_ball = np.mean(d_true_training[:, average_number_neighbors - 1])
    print("d_ball", d_ball)
    w_true_test_training = (d_true_test_training < d_ball).astype(np.int)
    print("w_true_test_training", w_true_test_training)


___________________________________________________________________________________________________________

  # # Calculate d_ball defned by training set in batches:
    # number_of_batches_training_set = 5
    # d_ball = 0.0
    # times = 0
    # data_train_norm_batches = np.array_split(data_train_norm, number_of_batches_training_set)
    # for tb_i, training_batch in enumerate(data_train_norm_batches):
    #     d_true_training_batch = distance.cdist(training_batch, data_train_norm, 'euclidean')
    #     d_true_training_batch.sort(axis=1)
    #     d_ball += np.sum(d_true_training_batch[:, average_number_neighbors - 1])
    #     times += d_true_training_batch.shape[0]
    #     # print("d_ball", d_ball)
    # d_ball = d_ball / (times * 1.0)
    # print("d_ball", d_ball)


_______________________________________________________________________________________________________________
(OLD D_BALL COMPUTATION)



    # # print("start calculating ground truth")
    # d_true_training = distance.cdist(data_train_norm, data_train_norm, 'euclidean')
    # d_true_training.sort(axis=1)
    # d_ball = np.mean(d_true_training[:, average_number_neighbors - 1])
    # print("d_ball", d_ball)


_________________________________________________________________________________________
plotly credentials:
# IrinaLuca
# 3NTZYCMEYnBD1yRxPWwq
# python -c "import plotly; plotly.tools.set_credentials_file(username='IrinaLuca', api_key='3NTZYCMEYnBD1yRxPWwq')"

_________________________________________________________________________________________________


def bit2int(bit_list):
    out = 0
    for bit in bit_list:
        out = (out << 1) | bit
    return out

import numpy as np

m = np.array([[1,0,0,1], [0,1,1,0]])
print np.vstack(map(bit2int, m))


__________________________________________________________




# def compress_dataset__balanced_partitioning(data_train_norm, data_test_norm, sh_model, dataset_label, condition_ordered_pcs):
#     print("\n# -- BALANCED, AS INITIALLY INTENDED: {0} set -- #".format(dataset_label))
#     # -- Define some params -- #
#     corner_case_num_buckets = sh_model.n_bits  # * 128
#
#     # -- Get dataset dimensions -- #
#     data_train_norm_n, data_train_norm_d = data_train_norm.shape
#     data_test_norm_n, data_test_norm_d = data_test_norm.shape
#
#     # -- PCA the given dataset according to the training set principal components -- #
#     data_train_norm_pcaed = data_train_norm.dot(sh_model.pc_from_training)
#     data_test_norm_pcaed = data_test_norm.dot(sh_model.pc_from_training)
#
#     # -- Move towards the actual compression -- #
#     data_train_norm_pcaed_and_centered = data_train_norm_pcaed - np.tile(sh_model.mn, (data_train_norm_n, 1))
#     data_test_norm_pcaed_and_centered = data_test_norm_pcaed - np.tile(sh_model.mn, (data_test_norm_n, 1))
#
#     # -- Loop through pcs as vanilla does, in the modes kind of order -- #
#     pcs_to_loop_through = enumerate(range(0, sh_model.n_bits))
#
#     # -- Create data box -- #
#     data_box_train = get_sine_data_box(data_train_norm_pcaed_and_centered, sh_model, data_train_norm_n)
#     data_box_test = get_sine_data_box(data_test_norm_pcaed_and_centered, sh_model, data_test_norm_n)
#
#     # -- Find out how many bits each pc contributes with -- #
#     bits_per_pcs = get_pc_bitwise_contribution(sh_model)
#     # print("\nbits_per_pcs: {0}".format(bits_per_pcs))
#     pcs_ith_bits_mapping, pcs_ith_bits_when_multiple_cuts, first_pcs_when_axis_cut_multiple_times = get_pcs_ith_bits_mapping(sh_model)
#     pcs_we_store_bits_for = [tup[1] for tth, tup in enumerate(pcs_ith_bits_when_multiple_cuts)]
#
#     if dataset_label == "testing":
#         print_help("pcs_we_store_bits_for", pcs_we_store_bits_for)
#         print("\npcs_ith_bits_mapping: {0}".format(pcs_ith_bits_mapping))
#         print("\npcs_to_loop_through: {0}".format(range(0, sh_model.n_bits)))
#         print("\npcs_ith_bits_when_multiple_cuts: {0}".format(pcs_ith_bits_when_multiple_cuts))
#         print("\nfirst_pcs_when_axis_cut_multiple_times: {0}".format(first_pcs_when_axis_cut_multiple_times))
#
#     if dataset_label == "training":
#         data_hashcodes = [[] for _ in range(0, data_train_norm_n)]
#         bits_stored_for_later = [[] for _ in range(0, data_train_norm_n)]
#     else:
#         data_hashcodes = [[] for _ in range(0, data_test_norm_n)]
#         bits_stored_for_later = [[] for _ in range(0, data_test_norm_n)]
#
#
#     # total_bits = 0
#     grey_codes_per_pc = {}
#     for pth, pc in pcs_to_loop_through:
#         # -- Establish n_buckets and n_bits -- #
#         num_bits_of_contribution = len(pcs_ith_bits_mapping[str(pc)]) if len(pcs_ith_bits_mapping[str(pc)]) > 0 else 1
#
#         # Obs(*): In case num_buckets_per_pc, it means we have way too many buckets and their int value overflows
#         num_buckets_per_pc = corner_case_num_buckets if np.power(2, num_bits_of_contribution) == 0 else np.power(2, num_bits_of_contribution)
#
#         # -- Establish box information based only on training -- #
#         pc_scores_train, max_pc_score_train, min_pc_score_train, range_pc_train, interval_pc_train = get_data_box_info(
#             data_box_train, pc, num_buckets_per_pc)
#
#         # -- GreyCode stuff -- #
#         gray_codes_pc = list(GrayCode(num_bits_of_contribution).generate_gray())
#         grey_codes_per_pc[str(pc)] = gray_codes_pc
#         num_gray_codes = len(gray_codes_pc)
#
#         # -- Establish pc scores/actual scores/data box scores which is about to be partitioned -- #
#         pc_scores = data_box_train[:, pc] if dataset_label == "training" else data_box_test[:, pc]
#
#         # -- Go through each data point in my pc box and check only the score corresponding to that pc dimension -- #
#         for dp, pc_score in enumerate(pc_scores):
#             if pc not in pcs_we_store_bits_for or len(pcs_ith_bits_mapping[str(pc)]) > 1:  #  pc in first_pcs_when_axis_cut_multiple_times or len(pcs_ith_bits_mapping[str(pc)]) <= 1:
#
#                 # if dataset_label == "testing" and dp == 47:
#                 #     print("PC={0}, COND 1".format(pc))
#
#                 provenance_bucket_index = get_provenance_bucket_index(pc_score, min_pc_score_train, interval_pc_train, num_gray_codes)
#                 bits = [int(bit_str) for bit_str in gray_codes_pc[provenance_bucket_index]]
#                 bit_to_attach = bits[0]
#
#                 if num_bits_of_contribution > 1:
#                     pcs_to_distribute_bits_to = pcs_ith_bits_mapping[str(pc)]
#                     pc_bit_paired_up = [(the_pc, the_bit) for the_pc, the_bit in zip(pcs_to_distribute_bits_to, bits)]
#                     if pc_bit_paired_up not in bits_stored_for_later[dp]:
#                         bits_stored_for_later[dp].extend(pc_bit_paired_up)
#
#                     # if dataset_label == "testing" and dp == 47:
#                     #     print("STORED bits for pc={0}, for these pcs={1}".format(pc, pcs_to_distribute_bits_to))
#                     #     print("")
#             else:
#                 bit_to_attach = [tup[1] for tup in bits_stored_for_later[dp] if tup[0] == pc][0]
#                 # if dataset_label == "testing" and dp == 47:
#                 #     print("PC={0}, COND 2".format(pc))
#                 #     test = [tup[1] for tup in bits_stored_for_later[dp] if tup[0] == pc]
#                 #     # print("pc={0}, bits_stored_for_later[dp]={1}".format(pc, bits_stored_for_later[dp]))
#                 #     print("IRINA test", test)
#                 #     if len(test) < 1:
#                 #         print("ALERT, not found bit stored before for pc={0}".format(pc))
#                 #     print("")
#                 # bit_to_attach = 1
#
#             data_hashcodes[dp] = np.hstack((data_hashcodes[dp], [bit_to_attach]))
#
#
#
#     u = np.array(data_hashcodes, dtype=bool)
#     # temp = u[:, 0]
#     # u[:, 0] = u[:, 1]
#     # u[:, 1] = temp
#     u_compactly_binarized = compact_bit_matrix(u)
#
#     # print_help("total_bits", total_bits)
#     # if dataset_label == "testing":
#     #     print_help("u[47]", u[47].astype(int))
#     #     print_help("len(u[47])", len(u[47].astype(int)))
#
#     return u, u_compactly_binarized
